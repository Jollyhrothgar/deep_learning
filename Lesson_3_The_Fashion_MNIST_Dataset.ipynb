{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 3: The Fashion MNIST Dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBTQx9WM+O20lkfDKUy9XK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jollyhrothgar/deep_learning/blob/master/Lesson_3_The_Fashion_MNIST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ROnOsInvIg4",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The Fashion MNIST dataset contains 70,000  images containing 10 different classes of clohting. Each clothing item is represented as a well-centered 28 by 28 grescale picture.\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "Each image is converted to a vector by flattening the image from a 28 $\\times$ 28 to a 784 element vector.\n",
        "\n",
        "The input layer will transform the vector into a densly connected 128 neuron output. This means that we multiply the 784 element vector representing an image with a 784 $\\times$ 128 matrix to generate a new transformed vector representing the embedding.\n",
        "\n",
        "For the activation function for each number, we use the relu function, which is useful in this case. ReLU has some interesting properties which make it useful here:\n",
        "\n",
        "* When the weighted sum of the neuron is less than 0, we set this to zero, effectively de-activating any subsequent weight associated with this neuron.\n",
        "\n",
        "There are lots of activation functions, and the properties of these functions tend to be designed to facilitate a few different things:\n",
        "\n",
        "* Computation constraint when calculating the gradient during back propagation / loss minimization\n",
        "* Try to solve the \"vanishing gradient\" problem - e.g., issues that create an ambiguous loss surface.\n",
        "* Avoid \"dead\" neurons - e.g. neurons for whom the learned weight is zero\n",
        "\n",
        "Activation functions can also be used map the final layer of the problem to classification or regression. ReLU should not be used for the final layer of a network - [more information here](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/#:~:text=The%20ReLU%20function%20is%20another,neurons%20at%20the%20same%20time.)\n",
        "\n",
        "Output layer for classification should be dense, and have the same number of neurons as there are classes. Additionally, the layer must use softmax activation, which converts the output to a probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvnJ_EZ2vtN3",
        "colab_type": "text"
      },
      "source": [
        "# Load Libraries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lt2esWZ0q33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "e99cf065-63d1-44a7-9f73-430797f04fab"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pretty progress bar\n",
        "import tqdm\n",
        "import tqdm.auto\n",
        "tqdm.tqdm = tqdm.auto.tqdm\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fefa8c3ba422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQFQ_Ugg1SsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}