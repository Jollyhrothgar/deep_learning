{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Lesson 1 - Set up your environment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZ+ltJJi6OijYpDDN9wKc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jollyhrothgar/deep_learning/blob/master/Deep_Learning_Lesson_1_Set_up_your_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBtGXWJ23CP5",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "To get this working on linux with a graphics card that supports CUDA accelleration, tensorflow, etc, we have to do many annoying steps, which I have attempted to document here.\n",
        "\n",
        "## Step 1 - Install the latest Nvidia Drivers from the \"Proprietary Drives\" Setting in Ubuntu\n",
        "\n",
        "* Confirm its working by running\n",
        "\n",
        "```bash\n",
        "nvidia-smi\n",
        "```\n",
        "\n",
        "To get an output such as:\n",
        "\n",
        "```\n",
        "Mon Aug 31 16:41:35 2020       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n",
        "|  0%   32C    P8    14W / 250W |    510MiB / 11176MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                       GPU Memory |\n",
        "|  GPU       PID   Type   Process name                             Usage      |\n",
        "|=============================================================================|\n",
        "|    0      1359      G   /usr/lib/xorg/Xorg                            24MiB |\n",
        "|    0      1496      G   /usr/bin/gnome-shell                          50MiB |\n",
        "|    0      2406      G   /usr/lib/xorg/Xorg                           145MiB |\n",
        "|    0      2544      G   /usr/bin/gnome-shell                         152MiB |\n",
        "|    0      4018      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   133MiB |\n",
        "+-----------------------------------------------------------------------------+\n",
        "\n",
        "```\n",
        "## Step 2 - Set up docker, make sure your local user is added to the docker group.\n",
        "\n",
        "## Step 3 - Get the NVIDIA docker container from tensorflow\n",
        "\n",
        "[Tensorflow](https://www.tensorflow.org/install/docker) has documentation for accomplishing this, but the quick version is:\n",
        "\n",
        "```bash\n",
        "docker pull tensorflow/tensorflow:latest-gpu-jupyter\n",
        "```\n",
        "\n",
        "## Step 4 - Prepare / persist shit between runs so you can connect to colab\n",
        "\n",
        "\n",
        "This gets run once:\n",
        "```bash\n",
        "docker run -ti --gpus all --rm -u $(id -u):$(id -g) -v \"${HOME}/.config/jupyter:/.jupyter\" tensorflow/tensorflow:latest-gpu-jupyter bash -c \"source /etc/bash.bashrc && jupyter serverextension enable --py jupyter_http_over_ws\"\n",
        "```\n",
        "\n",
        "## Step 5 - Run the docker container with the config:\n",
        "\n",
        "```bash\n",
        "docker run -it -u $(id -u):$(id -g) --gpus all -p 8888:8888 -v \"${HOME}/.config/jupyter:/.jupyter\" -v \"${HOME}/workspace:/tf\" tensorflow/tensorflow:latest-gpu-jupyter bash -c \"source /etc/bash.bashrc && jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --NotebookApp.allow_origin='https://colab.research.google.com'\"\n",
        "```\n",
        "\n",
        "Obviously, you have to have your directory structure set up - for example, directorys like `${HOME}/workspace` and `${HOME}/.config/jupyter` have to exist.\n",
        "\n",
        "\n",
        "## Step 6 Connect Local Runtime to Colab\n",
        "\n",
        "Go to colab, and connect to a local runtime, pasting the string that looks like this:\n",
        "\n",
        "`http://127.0.0.1:8888/?token=<long alphanumeric token>` into place for the backend URL, but replace `127.0.0.1` with `localhost`, since you tunneled the port earlier when you wrote `-p 8888:8888` in your docker run command.\n",
        "\n",
        "## Step 7 - Make an executable to run your docker setup so you don't have to remember this shit.\n",
        "\n",
        "## Step 8 - Run these commands with your local runtime connected.\n",
        "\n",
        "You should see something that indicates your GPU is connected like:\n",
        "\n",
        "`[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]`\n",
        "\n",
        "## Step 9 - Write Down the Useful Links you Found\n",
        "\n",
        "* https://jupyter-docker-stacks.readthedocs.io/en/latest/using/running.html\n",
        "* https://docs.docker.com/engine/reference/run/\n",
        "* https://stackoverflow.com/questions/61024722/how-to-use-google-colab-with-a-local-tensorflow-jupyter-server-using-powershell\n",
        "* https://github.com/tensorflow/tensorflow/issues/25247#issuecomment-459644861\n",
        "* https://research.google.com/colaboratory/local-runtimes.html\n",
        "\n",
        "## Step 10: Vomit From Exhaustion\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ob8E--4Xxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDcv1Gct5LL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a178344c-19b1-48d4-bf16-81ba6821defc"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjHd5_ks8Y8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}